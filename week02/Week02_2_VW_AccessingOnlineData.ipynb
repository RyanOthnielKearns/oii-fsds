{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Fundamentals of Social Data Science \n",
    "# Accessing data through the web\n",
    "\n",
    "Data comes to us in many ways, but commonly through the web, either through its inherent accessibility or through a specific set of queries. Working with data on the web is an important skill and requires knowledge of several interrelated technologies that work together to send and receive data. \n",
    "\n",
    "Key concepts in here are 'requests', 'responses', 'URLs', and 'apis'. In general to access data on the web we make a request to a specific URL and receive a response. If the URL returns machine-formatted data it is typically an API or Application Programming Interface. Below let's explore these topics in turn, starting with the URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket \n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## A URL or Uniform Resource Locator\n",
    "\n",
    "Uniform resource locators are addresses that connect the World Wide Web. They are a subset of URIs which include other sorts of addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?client=firefox-b-d&q=Why+are+red+M%26M%27s+%231%3F\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://www.google.com/search\"\n",
    "params = {\"client\":\"firefox-b-d\", \"q\":\"Why are red M&M's #1?\"}\n",
    "\n",
    "req = requests.models.PreparedRequest()\n",
    "req.prepare_url(URL, params)\n",
    "print(req.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see how a URL is prepared, which includes an _encoding_. It also includes parameters that can be included in the URL string. These parameters are used by the server to understand the request. They sometimes include important details like authentication keys, user_ids, and trackers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain names and IP addresses \n",
    "\n",
    "These web addresses include domain names (like www.google.com), paths and parameters. The domain name is resolved by an external server and translated into an IP address. This is then used to connect the client making the request and the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example IP address for Google.com: 142.250.200.46\n",
      "IP for localhost: 127.0.0.1\n",
      "My machine's IP Address on the local network: 127.0.0.1\n",
      "My machine's IP Address to others on the web: 192.xxx.xxx.xxx\n"
     ]
    }
   ],
   "source": [
    "#1. External server IP\n",
    "print(\"Example IP address for Google.com:\", socket.gethostbyname(\"google.com\"))\n",
    "\n",
    "#2. Localhost IP\n",
    "print(\"IP for localhost:\", socket.gethostbyname(\"localhost\"))\n",
    "\n",
    "#3. Local network IP\n",
    "local_ip = socket.gethostbyname(socket.gethostname())\n",
    "print(\"My machine's IP Address on the local network:\", local_ip)\n",
    "\n",
    "#4. External computer IP: Actual IP commented for privacy\n",
    "my_external_ip = requests.get('https://api.ipify.org').text\n",
    "print(\"My machine's IP Address to others on the web:\",\n",
    "      my_external_ip.split('.')[0] + \".xxx.xxx.xxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests and responses \n",
    "\n",
    "To get data from the web we can request it. There are two standard types of requests: GET and POST. GET sends an address to the web, which is then resolved to an IP address. POST additionally includes a payload. POST over HTTPS means that the payload is encrypted. Without it we might not be doing much shopping or email on the web.  That being said, we will not see POST requests below, but they may occur in later work.\n",
    "\n",
    "The GET request includes some details such as your IP address so that it knows where to return the data. Much of this is included in a header. Importantly the header also identifies the requesting agent, whether it is Python or Firefox. You can change this agent, and thus it is not very secure but it is mainly used to format a page for specific circumstances.  \n",
    "\n",
    "What is returned from a server is the 'response'. This includes both the response header and the response body. You can see both of these things in Python, but also in a web browser. I will show this in Firefox first and then let's look at some code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://consent.google.com/ml?continue=https://www.google.com/search%3Fclient%3Dfirefox-b-d%26q%3DWhy%2Bare%2Bred%2BM%2526M%2527s%2B%25231%253F&gl=GB&m=0&pc=srp&uxe=none&cm=2&hl=en&src=1\n",
      "{'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': '*/*', 'Connection': 'keep-alive', 'Cookie': 'AEC=AVYB7crGYJ-Qd64obaSD6ecyxppA72o2xI0Wp46buBjLEXQ8BphjT0HbVg; __Secure-ENID=23.SE=J6eaGO_GhqHrASkHCbyHAaRejppHV4qDFS51z6wF2UD8HvedaTvoIh8AGRXs-ihC8EFgWVHdZwxob5WAbTO3yyUYeJOKZZNHZQ-oN6eLGAKg789uSQFaQ80uQSS3QE2CD0td5XjSdtIBbQf_qQFW2ieYcJ_UZV9JJg-ex42XYQnSD4kddRu4duaWvQxT11S0ZCaotyU'}\n"
     ]
    }
   ],
   "source": [
    "req = requests.get(URL, params)\n",
    "\n",
    "# The actual URL that is returned from the server based on what we sent.\n",
    "print(req.request.url)\n",
    "\n",
    "# The headers that we sent to the server\n",
    "print(req.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we would generally only be concerned with the response body, sometimes the response header can provide useful information. For example, back when we were collecting tweet data, the resopnse header would include details related to 'rate limiting'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Headers:\n",
      "{'Content-Type': 'text/html; charset=utf-8', 'Cache-Control': 'no-cache, no-store, max-age=0, must-revalidate', 'Pragma': 'no-cache', 'Expires': 'Mon, 01 Jan 1990 00:00:00 GMT', 'Date': 'Wed, 23 Oct 2024 09:15:24 GMT', 'Content-Security-Policy': \"script-src 'nonce-2qVVLnr5anwtWuPenJoUhg' 'unsafe-inline';object-src 'none';base-uri 'self';report-uri /_/ConsentHttp/cspreport;worker-src 'self', require-trusted-types-for 'script';report-uri /_/ConsentHttp/cspreport\", 'Cross-Origin-Opener-Policy': 'unsafe-none', 'Cross-Origin-Resource-Policy': 'same-site', 'Accept-CH': 'Sec-CH-UA-Arch, Sec-CH-UA-Bitness, Sec-CH-UA-Full-Version, Sec-CH-UA-Full-Version-List, Sec-CH-UA-Model, Sec-CH-UA-WoW64, Sec-CH-UA-Form-Factors, Sec-CH-UA-Platform, Sec-CH-UA-Platform-Version', 'Permissions-Policy': 'ch-ua-arch=*, ch-ua-bitness=*, ch-ua-full-version=*, ch-ua-full-version-list=*, ch-ua-model=*, ch-ua-wow64=*, ch-ua-form-factors=*, ch-ua-platform=*, ch-ua-platform-version=*', 'reporting-endpoints': 'default=\"/_/ConsentHttp/web-reports?context=eJzjmsCoxSXF4K8hxXDTwZXh2GI3Bvarbgyzf7sxGIn7MJzP2MX02voWk8TXl0waQPz2iz_LBc0glrXZQSwiGaEsdStCWZg2hLKs2hjKIhMXxlLdEMbCOieM5cGKMBan9BmsQUDcevMc61QgXu5ynjXp33nWIiA2VLjE6gjEqj2XWE2B2K3lMmuRxBXWJiAW4uFY8_fRDjaBG5Ma_jEqaSflF8ZnpqTmlWSWVKYV5eeVpOallGYWpxaVpRbFGxkYmRgaGBnoGVjEFxgAAEJPTGs\"', 'Content-Encoding': 'gzip', 'Server': 'ESF', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'X-Content-Type-Options': 'nosniff', 'Set-Cookie': '__Secure-BUCKET=CLwG; Domain=.google.com; Expires=Mon, 21-Apr-2025 09:15:24 GMT; Path=/; Secure; HttpOnly; SameSite=lax', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000', 'Transfer-Encoding': 'chunked'}\n",
      "\n",
      "Response Body:\n",
      "<!DOCTYPE html><html lang=\"en\" dir=\"ltr\"><head><style nonce=\"TbMg-Ij29ZecsTXA3HGDcg\">\n",
      "a, a:link, a:visited, a:active, a:hover {\n",
      "  color: #1a73e8;\n",
      "  text-decoration: none;\n",
      "}\n",
      "body {\n",
      "  font-family: Roboto,Helvetica,Arial,sans-serif;\n",
      "  text-align: center;\n",
      "  -ms-text-size-adjust: 100%;\n",
      "  -moz-text-size-adjust: 100%;\n",
      "  -webkit-text-size-adjust: 100%;\n",
      "}\n",
      ".box {\n",
      "  border: 1px solid #dadce0;\n",
      "  box-sizing: border-box;\n",
      "  border-radius: 8px;\n",
      "  margin: 24px auto 5px auto;\n",
      "  max-width: 800px;\n",
      "  padding: 24px;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the response headers\n",
    "print(\"Response Headers:\")\n",
    "print(req.headers)\n",
    "\n",
    "# Display the response body\n",
    "print(\"\\nResponse Body:\")\n",
    "print(req.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs vs other requests \n",
    "\n",
    "An API is not a distinct thing but an ensemble of specific web addresses that are suitable for downloading machine formatted data. APIs may be authenticated or unauthenticated. When authenticated, the server knows precisely who is requesting the data and that can enable access to either more, more granular, or more personalised data. \n",
    "\n",
    "As an example, we can look at the Reddit API: https://www.reddit.com/dev/api/ Here we can see a list of 'endpoints'. These are specific web addresses such that if you go to these addresses it will return data in a machine format rather than a webpage. the `/api/v1/me` and so forth represent web addresses below the top level domain. Thus for the above, the actual address is `https://www.reddit.com/api/v1/me`. If you go to that address, however, it will likely say \"forbidden\". This is because the address is _authenticated_. We will cover that below, but first let's look at a simpler API on reddit. \n",
    "\n",
    "As we already saw briefly in class, data can come to us in a human readable or machine readable format, and that reddit will return data in `.json` format. For any given page, such as `https://www.reddit.com/r/advice` replacing `www` with `api` will format the page as JSON (`https://www.reddit.com/r/advice`). Thus, we can think of any reddit URL as an API endpoint. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs and Authentication \n",
    "\n",
    "To access a lot of information on the web one must be authenticated. Even with the Reddit API there are more limits to open access than to authenticated access. To authenticate to an API can take many forms. The most common although not necessarily the most modern standard is what's called \"OAuth\". This is a means of authenticating that can both allow a person to receive a 'token' or 'key' that signifies them but also can be done in such a way that these tokens can be passed to third parties. Why is this relevant? Imagine a webapp that renders Reddit. It will see your content. Should the app store your password? Should it store your password in such a way that the app maintainers know it? \n",
    "\n",
    "With OAuth, registration on Reddit is done exclusively with Reddit's servers. These servers then pass on a token so that the third party can act on your behalf. The third party shouldn't need to know your password or credentials. This means that it can work on your behalf when requested but that you would still be able to secure your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wrappers \n",
    "\n",
    "When we interact with APIs we can do this from scratch with basic Python. For an unauthenticated API, simply using the requests library would not be a challenge. But for authenticated APIs writing your own authentication can be a real nuisance. For the most part people draw upon the experience of others. For example, considering Reddit above, we can look to a wrapper called `praw` (the Python Reddit API Wrapper). \n",
    "\n",
    "This is a Python package that simplifies some of the challenges in collecting data through the API. This turns the steps of collecting this data which would with requests and responses into a more Pythonic format. \n",
    "\n",
    "If you do not have a Reddit account, you can create one for free with an email address. It is not necessary for collecting data, but it will be useful for apprecaiting the distinction between authenticated and non-authenticated APIs. For this class, we will not need to authenticate, but in your own work you may find that authenticating will permit access to different or more expansive data. For example, the Wikipedia API has different rate limits for logged in versus non-logged in users. Note that Reddit is in the process of shifting its API. As of this lecture, testing has revealed that on an active reddit account you can still access the tools to create an API-accessible script but less active and new accounts may be prevented. \n",
    "\n",
    "Thus, for the exercise below we will simply be using the https://api.reddit.com/ approach for instruction purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paging and requesting through multiple instances \n",
    "\n",
    "If an API returns data it often returns a subset of all possible data. In fact, if it returned all data it wouldn't be much of an API since it require you to subsequently process the data on your end. APIs can give more or less data than one asks for and we have to learn how to manage this. In the prior lab you were asked to discover and use the `--limit` parameter for the code from the Wikipedia scraper to increase the amount of pages requested. \n",
    "\n",
    "Below we will show how to use paging and some json parsing in order to collect multiple reddit pages from the webpage as API. This is generally an inefficient task and so one might either need to consider a long runway of time for collecting large volumes of data this way or another approach such as access to a more useful API. However, this does show paging and in the end we get some data we can put into a DataFrame. \n",
    "\n",
    "Recall the code for taking a response from Reddit in `.json`. We only looked at the json rows under `['data']['children']` which referred to the stories. But under the top level there was also an 'after' key. This key can be used as a parameter in the URL. Then the next set of entries will be all those up to the 'after' time. One can repeat this until they run out of data, time, or storage. In this case, we will simply take the first 4 iterations which is enough to show the point. \n",
    "\n",
    "Notice that below I use a _Session_. This is because we will want to keep persistent information between requests, such as the header.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "s = requests.Session()\n",
    "s.headers.update({'User-agent': 'Python/Hogan redditex 0.3'})\n",
    "\n",
    "req = s.get('https://api.reddit.com/r/aww/new')\n",
    "sample = json.loads(req.content)\n",
    "\n",
    "print(sample.keys(), sample['kind'], sample['data'].keys(),\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a Session object, we can use it to make multiple requests to the same server. This is useful when you need to maintain a session with a server. Below we will then use the session object to make a request to the same server. We include a `time.sleep` in order to ensure we do not hit Reddit's rate limits.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "after = \"\" # The first time is empty (and won't throw an error on Reddit)\n",
    "results = [] # The list of result objects\n",
    "count = 0  # Where we start\n",
    "max_count = 100 # Note: Reddit might not give results for maxcount > 1000\n",
    "\n",
    "while True:\n",
    "    reddit_link = f\"https://api.reddit.com/r/aww?after={after}\"\n",
    "\n",
    "    content = s.get(reddit_link)\n",
    "    if not content:\n",
    "        break\n",
    "\n",
    "    result = json.loads(req.content)\n",
    "    results.append(result[\"data\"][\"children\"])\n",
    "    count += len(result[\"data\"][\"children\"])\n",
    "\n",
    "    if count >= max_count: \n",
    "        break \n",
    "    else:\n",
    "        after = result[\"data\"][\"after\"]\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"We have received {len(results)} batches of data to be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together some of the skills covered already: json_normalise, concat, and dataframe management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.concat([pd.json_normalize(i) for i in results],\n",
    "                      ignore_index=True)\n",
    "\n",
    "reddit_df.columns = [i.removeprefix(\"data.\")\n",
    "                    if i.startswith(\"data.\") else i\n",
    "                    for i in reddit_df.columns]\n",
    "\n",
    "display(reddit_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data from Wikipedia \n",
    "\n",
    "Wikipedia is a vast and well used resource for data. It is used in the training of models and search engines. Many researchers have been fascinated by the behaviour of Wikipedians, their composition, and how articles themselves are reflective of society at large. It shows inequalities between the global north and south, inequalities in gender in online spaces with a substantial majority of editors being male, and it is a flashpoint for online trolling. \n",
    "\n",
    "In order to explore a Wikipedia page and it's change over time, we first would need to access the page. We can do this in a few different ways, which depend on our goals. \n",
    "\n",
    "Wikipedia has an API for querying data. That being said, it is a generally inefficient way to access the full history of a page. For that we might make use (as was done in the lab) of the 'special export' page. This is a specific endpoint that returns XML but can enable one to download a page in its entirety. For many tasks we might want to use the API, but for a complete dump, we would still consider special export. \n",
    "\n",
    "The lab today is to extend the OII-Wikipedia archive so that you can get a complete snapshot of a Wikipedia page using Special Export and place it in a DataFrame. The instructions are on Canvas but also available in an updated Wikipedia repository. How might you update your repository? That is included in the instructions in the lab. Feel free to download from Canvas. Or alternatively, read them in the repository on the web and follow the instructions: https://github.com/berniehogan/oii-fsds-wikipedia/blob/main/Week02_2_Lab_Wikipedia.ipynb \n",
    "\n",
    "While we are only expecting one group presentation on the Wikipedia archives, we would like everyone to be able to fetch from this repository and run the code to create a DataFrame themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Declaration \n",
    "\n",
    "Code today made extensive use of AI tools. This is the first day in which I'm using Github copilot. You can also make use of GitHub copilot for free by requesting the GitHub student pack: https://education.github.com/pack.\n",
    "\n",
    "The lab scripts were modified primarily by the updated Claude Sonnet 3.5 which provided astonishingly good and readable code. It went through approximately a dozen revisions for the edits to the download script with various testing and debugging to pare it down for simplicity as well as exploring options for pagination and APIs. In the end an architecture that involved a complete download from Wikipedia using the Special Export was considered the most prudent and least complicated option. Scripts were modified to employ multiple parameters. The additional features for summarisation by month in the download output, as well as the summaries of authors were Claude's editorial decisions which I kept. Claude also edited the readme file. You can notice in both the original script and the readme that the edits continued with the style of commenting and providing of options that were present in the original README. \n",
    "\n",
    "Small snippets were provided in text by Github copilot, which also drafted the instructions for the fetch / merging process for gathering upstream files. All code was tested and read carefully for extraneous or unexpected operations. While some of the remaining code was written from scratch much of it came from Chapters 5 and 6 of FSStDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
